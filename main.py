import asyncio
import json
from datetime import datetime
from core.logger import logger
from core.database import CacheDB
from core.models import ArticleModel
from modules.collector import DataCollector
from modules.analyst import IntelligenceAnalyst
from config.settings import settings

async def process_single_article(collector, analyst, db, entry, source):
    """å¤„ç†å•ç¯‡æ–‡ç« çš„åŸå­æ“ä½œ"""
    url = entry.link
    url_hash = collector._get_url_hash(url)
    
    # 1. ç¼“å­˜æ£€æŸ¥
    cached_summary = await db.is_processed(url_hash)
    if cached_summary:
        logger.info(f"âš¡ å‘½ä¸­ç¼“å­˜: {entry.title[:15]}")
        # å³ä½¿å‘½ä¸­ç¼“å­˜ï¼Œä¹Ÿæ„é€ æˆå¯¹è±¡è¿”å›ï¼Œä»¥ä¾¿ç”ŸæˆæŠ¥å‘Š
        return ArticleModel(
            title=entry.title, url=url, source_name=source['name'],
            category=source['category'], content="cached", summary=cached_summary
        )

    # 2. é‡‡é›†å†…å®¹
    content = await collector.extract_content(url)
    if not content:
        return None

    # 3. æ•°æ®éªŒè¯ä¸æ¸…æ´—
    try:
        article = ArticleModel(
            title=entry.title,
            url=url,
            source_name=source['name'],
            category=source['category'],
            content=content
        )
    except ValueError as e:
        logger.warning(f"æ•°æ®æ¸…æ´—è¿‡æ»¤: {e} - {url}")
        return None

    # 4. LLM æ‘˜è¦
    summary = await analyst.summarize(article.content)
    article.summary = summary

    # 5. å†™å…¥ç¼“å­˜
    await db.save_result(url_hash, url, article.title, summary)
    logger.success(f"âœ… å¤„ç†å®Œæˆ: {article.title[:15]}")
    
    return article

async def main():
    logger.info("ğŸš€ æ™ºèƒ½æƒ…æŠ¥ç³»ç»Ÿ v2.0 å¯åŠ¨")
    
    # åˆå§‹åŒ–
    db = CacheDB()
    await db.init_db()
    collector = DataCollector()
    analyst = IntelligenceAnalyst()
    
    # åŠ è½½æº
    with open(settings.SOURCES_PATH, 'r', encoding='utf-8') as f:
        sources = json.load(f)

    tasks = []
    
    # ç¬¬ä¸€é˜¶æ®µï¼šå¹¶å‘è·å– RSS åˆ—è¡¨
    logger.info("ğŸ“¡ æ­£åœ¨å¹¶å‘æ‰«æ RSS æº...")
    for source in sources:
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œç›´æ¥awaitï¼Œä¹Ÿå¯ä»¥gather
        entries = await collector.fetch_rss_entries(source)
        for entry in entries:
            tasks.append(process_single_article(collector, analyst, db, entry, source))

    # ç¬¬äºŒé˜¶æ®µï¼šå¹¶å‘å¤„ç†æ–‡ç«  (é‡‡é›† -> æ¸…æ´— -> æ‘˜è¦ -> ç¼“å­˜)
    logger.info(f"å³ä½¿å¤„ç†ä»»åŠ¡æ•°: {len(tasks)}")
    if not tasks:
        logger.warning("æ²¡æœ‰å‘ç°æ–°æ–‡ç« ")
        return

    results = await asyncio.gather(*tasks)
    valid_articles = [r for r in results if r is not None]

    # ç¬¬ä¸‰é˜¶æ®µï¼šç”ŸæˆæŠ¥å‘Š
    if valid_articles:
        logger.info("ğŸ“ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç®€æŠ¥...")
        # æå–æ‰€æœ‰æ‘˜è¦ç”¨äºç”Ÿæˆ Insight
        all_summaries = [a.summary for a in valid_articles]
        insight = await analyst.generate_insight(all_summaries)
        
        # æ¸²æŸ“
        report_content = analyst.render_report(valid_articles, insight)
        
        filename = f"output/Daily_Briefing_{datetime.now().strftime('%Y%m%d')}.md"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report_content)
            
        logger.success(f"ğŸ‰ æŠ¥å‘Šç”Ÿæˆå®Œæ¯•: {filename}")
    else:
        logger.info("æ— éœ€ç”ŸæˆæŠ¥å‘Š (æ— æœ‰æ•ˆæ•°æ®)")

if __name__ == "__main__":
    asyncio.run(main())